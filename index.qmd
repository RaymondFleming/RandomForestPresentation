---
title: "Random Forest"
author: "Kristen Monaco, Praya Cheekapara, Raymond Fleming, Teng Ma"
format: 
  revealjs: 
    theme: [simple, custom.scss]
    embed-resources: true
output: revealjs::revealjs_presentation
---

## TestSlide

```{r}
#| echo: FALSE
library(readr)
library(plyr)
library(ipred)
library(caret)
library(caTools)
library(randomForest)
library(ROSE)
library(ggplot2)
library(knitr)
```
```{r}
#| echo: TRUE
#| code-fold: TRUE


data <- read_csv("All_threat_data.csv")

ggplot(data, aes(x = factor(Status), fill = factor(Status))) + 
  geom_bar(show.legend = FALSE) +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "Barplot of Status", 
       x = "Status", 
       y = "Frequency") +
  theme_minimal() +
  theme(text = element_text(size = 12),
        plot.title = element_text(hjust = 0.5),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1),
        panel.grid.major = element_line(color = "grey80"),
        panel.grid.minor = element_blank())
```

## Random Forest Overview {.smaller}

- Ensemble machine learning method based on a large number of decision trees voting to predict a classification
- Benefits compared to decision tree:
    - Able to function with incomplete data
    -Lower likelihood of an overfit
    -Improved prediction accuracy


## Bootstrap Sampling (Bagging) {.smaller}

- Each decision tree uses a random sample of the original dataset
    - Using a subset of the dataset reduces the probability of an overfit model
    - Rows with missing data will often be left out of the sample, improving performance
    - Performed with replacement

## Random Feature Selection {.smaller}

:::: {.columns}

::: {.column width="50%"}
 - A random set of features is selected for each node in training
     - Information about feature importance may be saved and applies in future iterations
     - Even with automated random feature selection, feature selection and engineering prior to training may improve performance 
:::

:::{.column width="50%"}
```{r}
library(readr)
library(plyr)
library(ipred)
library(caret)
library(caTools)
library(randomForest)
library(ROSE)
library(ggplot2)
library(knitr)
library(rsample)     
library(dplyr)       
library(rpart)       
library(rpart.plot)  
library(ipred)      
library(caret)      
library(readr)
 
dataP2 <- read_csv("process.csv")
dataP2 <- as.data.frame(dataP2)
set.seed(123)
split2 <- sample.split(dataP2, SplitRatio = 0.7) 
species_train <- subset(dataP2, split2 == "TRUE") 
species_test <- subset(dataP2, split2 == "FALSE")

```

```{r}
#| echo: FALSE
#| code-fold: FALSE

ctrl <- trainControl(method = "cv",  number = 10) 

bagged_cv <- train(
  Group~ LF + GF + Biomes + Range +
    Habitat_degradation + Habitat_loss + IAS +
    Other + Unknown + Other + Over_exploitation,
  data    = species_train,
  method = "treebag",
  trControl = ctrl,
  importance = TRUE
)
 
plot(varImp(bagged_cv), 10)
```


:::


::::  
```{r}
#| echo: TRUE
#| code-fold: TRUE
#| output: FALSE
ctrl <- trainControl(method = "cv",  number = 10) 

bagged_cv <- train(
  Group~ LF + GF + Biomes + Range + Habitat_degradation +  
     Habitat_loss + IAS + Other + Unknown + Other + Over_exploitation,
  data    = species_train,
  method = "treebag",
  trControl = ctrl,
  importance = TRUE)

plot(varImp(bagged_cv), 10)
```

## Cross Validation {.smaller}


     
:::: {.columns}

::: {.column width="50%"}
 - Validation of performance of model
    - Resampling method similar to bootstrapping, but without replacement
    - Allows approximation of the general performance of a model

:::

:::{.column width="50%"}

```{r}
#| echo: FALSE
#| code-fold: FALSE

m3 <- rpart(
  formula = Group~ LF + GF + Biomes + Range +
    Habitat_degradation + Habitat_loss + IAS +
    Other + Unknown + Other + Over_exploitation,
  data    = species_train,
  method  = "anova"
)

rpart.plot(m3)

```

:::


::::  
```{r}
#| echo: TRUE
#| code-fold: TRUE
#| output: FALSE

 m3 <- rpart(
   formula = Group~ LF + GF + Biomes + Range +
     Habitat_degradation + Habitat_loss + IAS +
     Other + Unknown + Other + Over_exploitation,
   data    = species_train,
   method  = "anova"
 )
 rpart.plot(m3)
```

## Prediction {.smaller}

- Each trained decision tree produces its own prediction
    - Decision trees are independent, and were trained on different subsets of both data and features
     
     
   
     
## Ensemble Voting {.smaller}

- The results from each decision tree are combined into a voting classifier
    - The mode of the classification results will be the final prediction
     
## Dataset {.smaller}

- South African Red List
    - Data about plants with their habitat, traits, distribution, and factors influencing their current threatened/extinct status
- Purpose
    - Predict whether or not an unknown plant is threatened based on the above characteristics

## Visuals 1 {.smaller}

:::: {.columns}

::: {.column width="50%"}
 - Distribution Range
:::

:::{.column width="50%"}

```{r}
#| echo: false

library(readr)
library(plyr)
library(ipred)
library(caret)
library(caTools)
library(randomForest)
library(ROSE)
library(ggplot2)
library(knitr)


```
```{r}
#| echo: FALSE
#| code-fold: FALSE
ggplot(data = data, aes(x = Status, y = Range, fill = Status)) +
  geom_boxplot() +
  theme_bw() +
  ylim(0,100000)
```

:::


::::

```{r}
#| echo: TRUE
#| code-fold: TRUE
#| output: false
ggplot(data = data, aes(x = Status, y = Range, fill = Status)) +
  geom_boxplot() +
  theme_bw() +
  ylim(0,100000)
```

## Visuals 2 {.smaller}
:::: {.columns}

::: {.column width="50%"}
 - Cramer's V Association with Range binned into 20 categories
    - Target feature Group is most associated with Range, Family, Habitat Loss, Biome, and GF
    - The most associated features will likely be the most important features during model training
    - Colinearity does not appear to be present, further checks are
:::

:::{.column width="50%"}
```{r}
#| echo: false
#| 
library(tidyverse)
library(fastDummies)
library(caret)
library(randomForest)
library(corrplot)
library(DescTools)

train<-read.csv("All_threat_data.csv")

train$Group <- as.factor(train$Group)
train$LF <- as.factor(train$LF)
train$GF <- as.factor(train$GF)
train$Fam <- as.factor(train$Fam)
train$Biomes <- as.factor(train$Biomes)
train$Habitat_degradation <- as.factor(train$Habitat_degradation)
train$Habitat_loss <- as.factor(train$Habitat_loss)
train$IAS <- as.factor(train$IAS)
train$Other <- as.factor(train$Other)
train$Over_exploitation <- as.factor(train$Over_exploitation)
train$Pollution <- as.factor(train$Pollution)
train$Unknown <- as.factor(train$Unknown)
train$Status <- as.factor(train$Status)
corrDFRange <- train %>% select(Group, LF, GF, Biomes, Range, Habitat_degradation, Habitat_loss, IAS, Other, Over_exploitation, Pollution, Unknown)
```
```{r}
#| echo: FALSE
#| code-fold: TRUE
#Binning Range to make it categorical
corrDFRange <- corrDFRange %>% mutate(Range=ntile(Range, n=20))
corrplot::corrplot(DescTools::PairApply(corrDFRange,DescTools::CramerV), type='lower')
```
:::


::::
```{r}
#| echo: TRUE
#| code-fold: TRUE
#| output: false
#Binning Range to make it categorical
corrDFRange <- corrDFRange %>% mutate(Range=ntile(Range, n=20))
corrplot::corrplot(DescTools::PairApply(corrDFRange,DescTools::CramerV), type='lower')
```
 
## Analysis {.smaller}

 - 5 separate random forest models were created using separate methods of normalization
 
## Data Preparation {.smaller}

 - Preprocessing
    - Encode categorical features into numerical / factor features
    - Split the training set into a training and test set, avoiding class imbalance
     
## Preprocessing {.smaller}
 
 - Class Imbalance
    - Resample smaller classes in order to approximate equal classes
    - Training on imbalanced datasets will bias predictions to the larger class

## Normalization {.smaller}

::: {.panel-tabset}

### Min-Max
    
### Z-Score
    
### Max Absolute Value
    
### L1 Norm
    
### L2 Norm

:::
    

## Prediction {.smaller}

 - Combine results into a vector
 - Identify the most frequently predicted class
 - Iterate over entire test set, storing results
 - Generate a confusion matrix, calculate the sensitivity, and precision for each category
 - Iterate after tuning if necessary


## Results {.smaller}

 - Range was found to be the strongest predictor of extinction
 - Habitat loss is the second strongest predictor of extinction
  

